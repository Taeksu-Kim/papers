# Transfomer

## Abstract

#### 지배적 인 시퀀스 변환 모델은 인코더와 디코더를 포함하는 복잡한 반복 또는 컨벌루션 신경망을 기반으로합니다. 최고 성능의 모델은 또한 어텐션 메커니즘을 통해 인코더와 디코더를 연결합니다. 우리는 어텐션 메커니즘만을 기반으로하는 새로운 단순 네트워크 아키텍처 인 Transformer를 제안합니다. 이는 전적으로 반복과 컨볼 루션을 제거합니다. 두 가지 기계 번역 작업에 대한 실험을 통해 이러한 모델은 품질이 우수하면서도 병렬화가 가능하고 학습 시간이 크게 단축되는 것으로 나타났습니다. 우리의 모델은 WMT 2014 영어-독일어 번역 작업에서 28.4 BLEU를 달성하여 앙상블을 포함한 기존 최고의 결과를 2BLEU 이상 향상 시켰습니다. WMT 2014 영어-프랑스어 번역 작업에서, 우리 모델은 8 개의 GPU에서 3.5 일 동안 훈련 한 후 새로운 단일 모델 최신 BLEU 점수 41.8을 설정합니다. 이는 최고의 훈련 비용의 일부입니다. 문학의 모델. Transformer가 크고 제한된 학습 데이터를 사용하여 영어 구성 구문 분석에 성공적으로 적용하여 다른 작업에 잘 일반화됨을 보여줍니다.


## 1 Introduction

#### 특히 반복 신경망, 장단기 기억 [13] 및 게이트 된 반복 신경망 [7]은 언어 모델링 및 기계 번역과 같은 시퀀스 모델링 및 변환 문제에서 최첨단 접근 방식으로 확고하게 확립되었습니다 [35, 2 , 5]. 그 이후로 반복되는 언어 모델과 인코더-디코더 아키텍처의 경계를 계속 확장하기위한 수많은 노력이 계속되었습니다 [38, 24, 15]. 반복 모델은 일반적으로 입력 및 출력 시퀀스의 기호 위치를 따라 계산을 인수 분해합니다. 계산 시간의 단계에 위치를 정렬하면 이전 숨겨진 상태 ht-1 및 위치 t에 대한 입력의 함수로 숨겨진 상태 시퀀스 ht를 생성합니다. 이러한 본질적으로 순차적 인 특성은 메모리 제약으로 인해 여러 예제에서 일괄 처리를 제한하기 때문에 긴 시퀀스 길이에서 중요 해지는 학습 예제 내에서 병렬화를 방지합니다. 최근 연구는 인수 분해 트릭 [21]과 조건부 계산 [32]을 통해 계산 효율성을 크게 향상 시켰으며 후자의 경우 모델 성능을 개선했습니다. 그러나 순차 계산의 근본적인 제약은 여전히 ​​남아 있습니다. 주의 메커니즘은 다양한 작업에서 강력한 시퀀스 모델링 및 변환 모델의 필수 부분이되어 입력 또는 출력 시퀀스의 거리에 관계없이 종속성을 모델링 할 수 있습니다 [2, 19]. 그러나 몇몇 경우를 제외한 모든 경우 [27], 이러한주의 메커니즘은 반복 네트워크와 함께 사용됩니다. 이 작업에서 우리는 반복을 피하고 대신에 전적으로주의 메커니즘에 의존하여 입력과 출력 사이의 전역 종속성을 그리는 모델 아키텍처 인 Transformer를 제안합니다. Transformer는 훨씬 더 많은 병렬화를 허용하며 8 개의 P100 GPU에서 12 시간 동안 교육을받은 후 번역 품질면에서 새로운 상태에 도달 할 수 있습니다.

## 2 Background

#### 순차 계산을 줄이는 목표는 확장 신경 GPU [16], ByteNet [18] 및 ConvS2S [9]의 기반을 형성합니다. 이들 모두는 컨볼 루션 신경망을 기본 빌딩 블록으로 사용하고 모든 입력 및 모든 입력에 대해 병렬로 숨겨진 표현을 계산합니다. 출력 위치. 이러한 모델에서 두 임의의 입력 또는 출력 위치에서 신호를 연결하는 데 필요한 작업 수는 위치 간 거리에서 ConvS2S의 경우 선형으로, ByteNet의 경우 대수적으로 증가합니다. 이것은 먼 위치 사이의 의존성을 배우는 것을 더 어렵게 만듭니다 [12]. Transformer에서는 평균주의 가중치 위치로 인해 유효 해상도가 감소하지만 섹션 3.2에 설명 된 것처럼 Multi-Head Attention에 대응하는 효과가 있지만 이는 일정한 수의 작업으로 감소됩니다. 내부주의라고도하는 자기주의는 시퀀스의 표현을 계산하기 위해 단일 시퀀스의 다른 위치와 관련된주의 메커니즘입니다. 자기주의는 독해, 추상적 인 요약, 텍스트 수반 및 학습 과제 독립적 문장 표현을 포함한 다양한 작업에서 성공적으로 사용되었습니다 [4, 27, 28, 22]. 엔드-투-엔드 메모리 네트워크는 순차 정렬 된 반복 대신 반복적 인주의 메커니즘을 기반으로하며 간단한 언어 질문 답변 및 언어 모델링 작업에서 잘 수행되는 것으로 나타났습니다 [34]. 그러나 우리가 아는 한, Transformer는 시퀀스 정렬 된 RNN 또는 컨볼 루션을 사용하지 않고 입력 및 출력의 표현을 계산하기 위해 전적으로 자기주의에 의존하는 최초의 변환 모델입니다. 다음 섹션에서는 Transformer에 대해 설명하고 자기주의를 유도하며 [17, 18] 및 [9]와 같은 모델에 비해 장점에 대해 논의합니다.

## 3 Model Architecture

#### 대부분의 경쟁적인 신경 시퀀스 변환 모델은 인코더-디코더 구조를 가지고 있습니다 [5, 2, 35]. 여기서 인코더는 기호 표현의 입력 시퀀스 (x1, ..., xn)를 연속 표현 시퀀스 z = (z1, ..., zn)에 매핑합니다. z가 주어지면 디코더는 한 번에 한 요소 씩 심볼의 출력 시퀀스 (y1, ..., ym)를 생성합니다. 각 단계에서 모델은 자동 회귀 (auto-regressive) [10]로 다음을 생성 할 때 이전에 생성 된 기호를 추가 입력으로 사용합니다. Transformer는 그림 1의 왼쪽 및 오른쪽 절반에 각각 표시된 인코더와 디코더 모두에 대해 누적 된 자기주의 및 포인트 방식의 완전 연결 계층을 사용하여이 전체 아키텍처를 따릅니다.

![image](https://user-images.githubusercontent.com/63130907/123031583-752ead00-d41f-11eb-93fc-684bfbe7d2e6.png)


### 3.1 Encoder and Decoder Stacks

#### 인코더 : 인코더는 N = 6 개의 동일한 레이어 스택으로 구성됩니다. 각 레이어에는 두 개의 하위 레이어가 있습니다. 첫 번째는 다중 헤드 자체주의 메커니즘이고 두 번째는 단순하고 위치별로 완전히 연결된 피드 포워드 네트워크입니다. 우리는 두 하위 계층 각각 주위에 잔류 연결 [11]을 사용하고 계층 정규화 [1]를 따릅니다. 즉, 각 하위 계층의 출력은 LayerNorm (x + Sublayer (x))이며, 여기서 Sublayer (x)는 하위 계층 자체에 의해 구현 된 함수입니다. 이러한 잔여 연결을 용이하게하기 위해 모델의 모든 하위 계층과 임베딩 계층은 차원 dmodel = 512의 출력을 생성합니다.

#### 디코더 : 디코더는 N = 6 개의 동일한 레이어 스택으로 구성됩니다. 각 인코더 레이어에있는 두 개의 하위 레이어 외에도 디코더는 인코더 스택의 출력에 대해 다중 헤드주의를 수행하는 세 번째 하위 레이어를 삽입합니다. 인코더와 마찬가지로 각 하위 계층 주변에 잔류 연결을 사용한 다음 계층 정규화를 사용합니다. 또한 디코더 스택의 자기주의 하위 계층을 수정하여 위치가 후속 위치에 집중하지 않도록합니다. 이 마스킹은 출력 임베딩이 한 위치만큼 오프셋된다는 사실과 결합되어 위치 i에 대한 예측이 i보다 작은 위치에서 알려진 출력에만 의존 할 수 있도록합니다.

### 3.2 Attention

#### 주의 함수는 쿼리와 키-값 쌍 집합을 출력에 매핑하는 것으로 설명 할 수 있습니다. 여기서 쿼리, 키, 값 및 출력은 모두 벡터입니다. 출력은 값의 가중 합계로 계산되며, 각 값에 할당 된 가중치는 해당 키와 쿼리의 호환성 함수에 의해 계산됩니다.

![image](https://user-images.githubusercontent.com/63130907/123031848-f71ed600-d41f-11eb-87b5-bc145a4fbd05.png)

### 3.2.1 Scaled Dot-Product Attention

#### 우리는 우리의 특별한 관심을 "Scaled Dot-Product Attention"이라고 부릅니다 (그림 2). 입력은 차원 dk의 쿼리 및 키와 차원 dv의 값으로 구성됩니다. 모든 키로 쿼리의 내적을 계산하고 각각을 √ dk로 나누고 소프트 맥스 함수를 적용하여 값에 대한 가중치를 얻습니다. 실제로 우리는 일련의 쿼리에 대한주의 함수를 동시에 계산하여 행렬 Q로 함께 압축합니다. 키와 값도 행렬 K 및 V로 함께 압축됩니다. 출력 행렬을 다음과 같이 계산합니다.

![image](https://user-images.githubusercontent.com/63130907/123031920-16b5fe80-d420-11eb-9c79-c6115ea4a30e.png)

#### 가장 일반적으로 사용되는 두 가지주의 함수는 덧셈주의 [2]와 내적 (곱셈)주의입니다. 내적주의는 √ 1 dk의 배율 인수를 제외하고는 우리 알고리즘과 동일합니다. 부가 적주의는 단일 히든 레이어가있는 피드 포워드 네트워크를 사용하여 호환성 함수를 계산합니다. 이 둘은 이론상의 복잡성이 비슷하지만 내적 관심은 고도로 최적화 된 행렬 곱셈 코드를 사용하여 구현할 수 있기 때문에 실제로 훨씬 빠르고 공간 효율적입니다. dk 값이 작은 경우 두 메커니즘이 비슷하게 수행되지만, dk 값이 더 큰 경우에는 가산 적주의가 내적주의를 능가합니다 [3]. 우리는 dk의 큰 값에 대해 내적의 크기가 커져서 소프트 맥스 함수를 극히 작은 기울기가있는 영역으로 밀어 넣는다 고 생각합니다 4. 이 효과에 대응하기 위해 내적을 √ 1 dk로 스케일합니다.

### 3.2.2 Multi-Head Attention

#### dmodel 차원 키, 값 및 쿼리를 사용하여 단일주의 기능을 수행하는 대신 학습 된 다른 선형 프로젝션을 사용하여 쿼리, 키 및 값을 각각 dk, dk 및 dv 차원으로 선형 프로젝션하는 것이 유리하다는 것을 알았습니다. 이러한 각 예상 버전의 쿼리, 키 및 값에서주의 기능을 병렬로 수행하여 dv 차원 출력 값을 산출합니다. 그림 2와 같이 연결되고 다시 투영되어 최종 값이 생성됩니다.

#### 다중 머리주의를 사용하면 모델이 서로 다른 위치에서 서로 다른 표현 부분 공간의 정보에 공동으로 참석할 수 있습니다. 하나의 어텐션 헤드로 평균화는 이것을 억제합니다.

![image](https://user-images.githubusercontent.com/63130907/123032106-60064e00-d420-11eb-8466-1112ec541427.png)

#### 투영이 매개 변수 행렬 인 경우 W Q i ∈ R dmodel × dk, W K i ∈ R dmodel × dk, WV i ∈ R dmodel × dv 및 WO ∈ R hdv × dmodel.
#### 이 작업에서 우리는 h = 8 개의 평행주의 레이어 또는 헤드를 사용합니다. 이들 각각에 대해 우리는 dk = dv = dmodel / h = 64를 사용합니다. 각 헤드의 감소 된 차원으로 인해 총 계산 비용은 완전한 차원의 단일 헤드주의 비용과 유사합니다.

### 3.2.3 Applications of Attention in our Model

#### Transformer는 세 가지 다른 방식으로 다중 머리주의를 사용합니다.
- "인코더-디코더주의"계층에서 쿼리는 이전 디코더 계층에서 제공되고 메모리 키 및 값은 인코더의 출력에서 제공됩니다. 이를 통해 디코더의 모든 위치가 입력 시퀀스의 모든 위치를 처리 할 수 있습니다. 이것은 [38, 2, 9]와 같은 sequence-to-sequence 모델에서 전형적인 인코더-디코더주의 메커니즘을 모방합니다.
- 인코더에는 자기주의 레이어가 포함되어 있습니다. 자기주의 계층에서 모든 키, 값 및 쿼리는 동일한 위치 (이 경우 인코더의 이전 계층 출력)에서 가져옵니다. 인코더의 각 위치는 인코더의 이전 계층에있는 모든 위치를 따를 수 있습니다.
- 유사하게, 디코더의 자기주의 계층은 디코더의 각 위치가 해당 위치를 포함하여 디코더의 모든 위치에 집중할 수 있도록합니다. 자동 회귀 속성을 유지하려면 디코더에서 왼쪽으로 정보 흐름을 방지해야합니다. 불법 연결에 해당하는 소프트 맥스의 입력에서 모든 값을 마스킹 (-∞로 설정)하여 스케일 된 내적주의 내부를 구현합니다. 그림 2 참조

### 3.3 Position-wise Feed-Forward Networks

#### 주의 하위 계층 외에도 인코더 및 디코더의 각 계층에는 완전히 연결된 피드 포워드 네트워크가 포함되어 있으며 각 위치에 개별적으로 동일하게 적용됩니다. 이것은 ReLU 활성화가 중간에있는 두 개의 선형 변환으로 구성됩니다. 

![image](https://user-images.githubusercontent.com/63130907/123033784-46b2d100-d423-11eb-85a5-4c3582c1d337.png)


#### 선형 변환은 다른 위치에서 동일하지만 레이어마다 다른 매개 변수를 사용합니다. 이것을 설명하는 또 다른 방법은 커널 크기가 1 인 두 컨볼 루션으로 입력 및 출력의 차원은 dmodel = 512이고 내부 계층의 차원은 df f = 2048입니다.

### 3.4 Embeddings and Softmax

#### 다른 시퀀스 변환 모델과 마찬가지로 학습 된 임베딩을 사용하여 입력 토큰과 출력 토큰을 차원 dmodel의 벡터로 변환합니다. 또한 일반적인 학습 선형 변환 및 softmax 함수를 사용하여 디코더 출력을 예측 된 다음 토큰 확률로 변환합니다. 우리 모델에서는 [30]과 유사하게 두 개의 임베딩 레이어와 pre-softmax 선형 변환 사이에 동일한 가중치 행렬을 공유합니다. 임베딩 레이어에서 가중치에 √ dmodel을 곱합니다.

### 3.5 Positional Encoding

#### 우리의 모델에는 반복과 컨볼 루션이 없기 때문에 모델이 시퀀스의 순서를 사용하려면 시퀀스에서 토큰의 상대적 또는 절대 위치에 대한 정보를 주입해야합니다. 이를 위해 인코더 및 디코더 스택의 하단에있는 입력 임베딩에 "위치 인코딩"을 추가합니다. 위치 인코딩은 임베딩과 동일한 차원 dmodel을 가지므로 둘을 합산 할 수 있습니다. 위치 인코딩의 많은 선택, 학습 및 고정이 있습니다 [9].

![image](https://user-images.githubusercontent.com/63130907/123034007-b1640c80-d423-11eb-83a0-d9018396da0a.png)

#### In this work, we use sine and cosine functions of different frequencies:

![image](https://user-images.githubusercontent.com/63130907/123034031-c2148280-d423-11eb-9835-412aab826e9c.png)

#### 여기서 pos는 위치이고 i는 차원입니다. 즉, 위치 인코딩의 각 차원은 정현파에 해당합니다. 파장은 2π에서 10000 · 2π로 기하학적 진행을 형성합니다. 고정 오프셋 k에 대해 P Epos + k는 P Epos의 선형 함수로 표현 될 수 있기 때문에 모델이 상대적 위치에 따라 쉽게 참석하는 방법을 배울 수 있다는 가설을 세웠 기 때문에이 함수를 선택했습니다. 또한 학습 된 위치 임베딩 [9]을 사용하여 실험 한 결과 두 버전이 거의 동일한 결과를 생성한다는 것을 발견했습니다 (표 3 행 (E) 참조). 모델이 훈련 중에 발생하는 길이보다 긴 시퀀스 길이로 외삽 할 수 있기 때문에 정현파 버전을 선택했습니다.

## 4 Why Self-Attention

#### 이 섹션에서는 자기주의 레이어의 다양한 측면을 심볼 표현의 하나의 가변 길이 시퀀스 (x1, ..., xn)를 동일한 길이의 다른 시퀀스 (z1, ..)에 매핑하는 데 일반적으로 사용되는 반복 및 컨볼 루션 레이어와 비교합니다. ., zn), xi, zi ∈ R d, 예를 들어 일반적인 시퀀스 변환 인코더 또는 디코더의 숨겨진 레이어. 자기주의를 사용하도록 동기를 부여하기 위해 세 가지 데시 데라 타를 고려합니다. 하나는 레이어 당 총 계산 복잡성입니다. 다른 하나는 필요한 최소 순차 작업 수로 측정 할 때 병렬화 할 수있는 계산량입니다. 세 번째는 네트워크에서 장거리 종속성 간의 경로 길이입니다. 장거리 의존성을 학습하는 것은 많은 시퀀스 변환 작업에서 핵심 과제입니다. 이러한 종속성을 학습하는 능력에 영향을 미치는 한 가지 핵심 요소는 순방향 및 역방향 신호가 네트워크에서 통과해야하는 경로의 길이입니다. 입력 및 출력 시퀀스의 위치 조합 사이의 이러한 경로가 짧을수록 장거리 종속성을 배우는 것이 더 쉽습니다 [12]. 따라서 다른 계층 유형으로 구성된 네트워크에서 두 입력 및 출력 위치 사이의 최대 경로 길이도 비교합니다. 표 1에서 알 수 있듯이 자기주의 계층은 모든 위치를 순차적으로 실행되는 일정한 수의 작업으로 연결하는 반면, 반복 계층은 O (n) 개의 연속 작업을 필요로합니다. 계산 복잡성 측면에서 자기주의 계층은 시퀀스 길이 n이 표현 차원 d보다 작을 때 반복 계층보다 빠릅니다. 이는 기계 번역의 최첨단 모델에서 사용되는 문장 표현에서 가장 자주 발생합니다. , 단어 조각 [38] 및 바이트 쌍 [31] 표현. 매우 긴 시퀀스를 포함하는 작업에 대한 계산 성능을 향상시키기 위해 각 출력 위치를 중심으로하는 입력 시퀀스에서 크기 r의 이웃 만 고려하는 것으로 자기주의를 제한 할 수 있습니다. 이것은 최대 경로 길이를 O (n / r)로 증가시킵니다. 향후 작업에서이 접근 방식을 추가로 조사 할 계획입니다. 커널 너비가 k <n 인 단일 컨벌루션 계층은 모든 쌍의 입력 및 출력 위치를 연결하지 않습니다. 이렇게하려면 연속 커널의 경우 O (n / k) 컨벌루션 레이어 스택이 필요하고 확장 된 컨볼 루션의 경우 O (logk (n)) [18], 두 위치 사이의 가장 긴 경로의 길이가 증가합니다. 네트워크에서. 컨볼 루션 레이어는 일반적으로 반복 레이어보다 k 배 더 비쌉니다. 그러나 분리 가능한 컨볼 루션 [6]은 복잡도를 O (k · n · d + n · d 2)로 상당히 감소시킵니다. 그러나 k = n을 사용하더라도 분리 가능한 컨볼 루션의 복잡성은 모델에서 취하는 접근 방식 인 자기주의 레이어와 포인트 단위 피드 포워드 레이어의 조합과 같습니다. 부수적 인 이점으로 자기주의는 더 해석 가능한 모델을 생성 할 수 있습니다. 우리는 모델의주의 분포를 검사하고 부록에서 예제를 제시하고 논의합니다. 개별주의 헤드는 서로 다른 작업을 수행하는 방법을 명확하게 배울뿐만 아니라 많은 문장의 구문 및 의미 구조와 관련된 행동을 보이는 것으로 보입니다.

## 5 Training
#### 이 섹션에서는 모델의 훈련 방식을 설명합니다.

### 5.1 Training Data and Batching
#### 우리는 약 450 만 문장 쌍으로 구성된 표준 WMT 2014 영어-독일어 데이터 세트에 대해 훈련했습니다. 문장은 약 37000 토큰의 공유 된 sourcetarget 어휘를 가진 바이트 쌍 인코딩 [3]을 사용하여 인코딩되었습니다. 영어-프랑스어의 경우, 우리는 36M 문장과 분할 토큰으로 구성된 훨씬 더 큰 WMT 2014 영어-프랑스어 데이터 세트를 32000 단어 조각 어휘로 사용했습니다 [38]. 문장 쌍은 대략적인 시퀀스 길이로 함께 일괄 처리되었습니다. 각 학습 배치에는 약 25000 개의 소스 토큰과 25000 개의 대상 토큰이 포함 된 문장 쌍 세트가 포함되어 있습니다.

### 5.2 Hardware and Schedule
#### 우리는 8 개의 NVIDIA P100 GPU가있는 하나의 컴퓨터에서 모델을 훈련했습니다. 백서 전체에 설명 된 하이퍼 파라미터를 사용하는 기본 모델의 경우 각 학습 단계에 약 0.4 초가 걸렸습니다. 총 100,000 단계 또는 12 시간 동안 기본 모델을 훈련했습니다. 우리의 큰 모델의 경우 (표 3의 하단에 설명 됨) 단계 시간은 1.0 초였습니다. 큰 모델은 300,000 보 (3.5 일) 동안 훈련되었습니다.

### 5.3 Optimizer
#### β1 = 0.9, β2 = 0.98 및 \ x0f = 10−9 인 Adam 옵티 마이저 [20]를 사용했습니다. 다음 공식에 따라 교육 과정에서 학습률을 다양하게 변경했습니다.
![image](https://user-images.githubusercontent.com/63130907/123034542-a9f13300-d424-11eb-9745-509d25defc37.png)

#### 이것은 첫 번째 warmup_steps 훈련 단계에 대해 선형 적으로 학습률을 증가시키고 이후 단계 수의 역 제곱근에 비례하여 감소시키는 것에 해당합니다. warmup_steps = 4000을 사용했습니다.

### 5.4 Regularization
#### 훈련 중에 세 가지 유형의 정규화를 사용합니다. :
#### 잔여 드롭 아웃 우리는 드롭 아웃 [33]을 각 하위 레이어의 출력에 적용하고, 하위 레이어 입력에 추가되고 정규화됩니다. 또한 인코더 및 디코더 스택 모두에서 임베딩 및 위치 인코딩의 합계에 드롭 아웃을 적용합니다. 기본 모델의 경우 Pdrop = 0.1 비율을 사용합니다.

![image](https://user-images.githubusercontent.com/63130907/123034723-03f1f880-d425-11eb-9046-0cbf7448fb07.png)

#### 레이블 평활화 훈련 중에 \ x0fls = 0.1 값의 레이블 평활화를 사용했습니다 [36]. 모델이 더 확실하지 않다는 것을 알게되지만 정확도와 BLEU 점수가 향상됨에 따라 이는 혼란을 해칩니다.

## 6 Results

### 6.1 Machine Translation
#### WMT 2014 영어-독일어 번역 작업에서 빅 트랜스포머 모델 (표 2의 트랜스포머 (big))은 이전에 가장 잘보고 된 모델 (앙상블 포함)보다 2.0 BLEU 이상 성능을 발휘하여 새로운 최신 상태를 설정합니다. 28.4의 예술 BLEU 점수. 이 모델의 구성은 표 3의 하단에 나열되어 있습니다. 8 개의 P100 GPU에서 훈련하는 데 3.5 일이 걸렸습니다. 우리의 기본 모델조차도 경쟁 모델의 교육 비용의 일부로 이전에 게시 된 모든 모델과 앙상블을 능가합니다. WMT 2014 영어-프랑스어 번역 작업에서 빅 모델은 BLEU 점수 41.0을 달성하여 이전에 게시 된 모든 단일 모델을 능가하며 이전 최첨단 교육 비용의 1/4 미만입니다. 모델. 영어에서 프랑스어로 학습 된 Transformer (대형) 모델은 0.3 대신 드롭 아웃 비율 Pdrop = 0.1을 사용했습니다. 기본 모델의 경우 10 분 간격으로 작성된 마지막 5 개의 체크 포인트를 평균하여 얻은 단일 모델을 사용했습니다. 대형 모델의 경우 마지막 20 개의 체크 포인트를 평균했습니다. 우리는 빔 크기가 4이고 길이 패널티 α = 0.6 인 빔 검색을 사용했습니다 [38]. 이러한 하이퍼 파라미터는 개발 세트에 대한 실험 후에 선택되었습니다. 추론하는 동안 최대 출력 길이를 입력 길이 + 50으로 설정했지만 가능하면 일찍 종료합니다 [38]. 표 2는 결과를 요약하고 번역 품질 및 교육 비용을 문헌의 다른 모델 아키텍처와 비교합니다. 훈련 시간, 사용 된 GPU 수, 각 GPU 5의 지속적인 단 정밀도 부동 소수점 용량 추정치를 곱하여 모델 훈련에 사용되는 부동 소수점 연산의 수를 추정합니다.

### 6.2 Model Variations
#### Transformer의 다양한 구성 요소의 중요성을 평가하기 위해 우리는 개발 세트 인 newstest2013에서 영어-독일어 번역의 성능 변화를 측정하여 다양한 방식으로 기본 모델을 변경했습니다. 이전 섹션에서 설명한대로 빔 검색을 사용했지만 체크 포인트 평균은 사용하지 않았습니다. 이러한 결과를 표 3에 제시합니다. 표 3 행 (A)에서 3.2.2 절에 설명 된대로 계산량을 일정하게 유지하면서주의 헤드 수와주의 키 및 값 차원을 변경합니다. 단일 헤드주의는 최상의 설정보다 0.9 BLEU 더 나쁘지만 헤드가 너무 많으면 품질도 떨어집니다.

![image](https://user-images.githubusercontent.com/63130907/123034986-88dd1200-d425-11eb-9213-f6f9428cad67.png)

![image](https://user-images.githubusercontent.com/63130907/123035032-a01bff80-d425-11eb-9f80-39a1ee889d2b.png)

표 3 행 (B)에서주의 키 크기 dk를 줄이면 모델 품질이 저하된다는 것을 알 수 있습니다. 이는 호환성을 결정하는 것이 쉽지 않으며 내적보다 더 정교한 호환성 기능이 도움이 될 수 있음을 시사합니다. 또한 행 (C)와 (D)에서 예상대로 더 큰 모델이 더 낫고 드롭 아웃이 과적 합을 피하는 데 매우 도움이된다는 것을 관찰합니다. 행 (E)에서 우리는 정현파 위치 인코딩을 학습 된 위치 임베딩 [9]으로 대체하고 기본 모델과 거의 동일한 결과를 관찰합니다.


### 6.3 English Constituency Parsing
#### Transformer가 다른 작업으로 일반화 할 수 있는지 평가하기 위해 영어 구성 구문 분석에 대한 실험을 수행했습니다. 이 작업은 특정 과제를 제시합니다. 산출물은 강력한 구조적 제약을 받고 투입물보다 훨씬 더 깁니다. 더욱이 RNN 시퀀스-투-시퀀스 모델은 작은 데이터 영역에서 최신 결과를 얻을 수 없었습니다 [37]. 우리는 Penn Treebank [25]의 월스트리트 저널 (WSJ) 부분에서 dmodel = 1024로 4 레이어 트랜스포머를 훈련했습니다. 우리는 또한 약 1,700 만 개의 문장으로 이루어진 더 큰 신뢰도와 BerkleyParser 말뭉치를 사용하여 준지도 환경에서 훈련했습니다 [37]. WSJ 전용 설정에는 16K 토큰 어휘를 사용하고 준 감독 설정에는 32K 토큰 어휘를 사용했습니다. 섹션 22 개발 세트에서 드롭 아웃,주의 및 잔여 (섹션 5.4), 학습률 및 빔 크기를 선택하기 위해 소수의 실험 만 수행했으며, 다른 모든 매개 변수는 영어-독일어 기본 번역 모델에서 변경되지 않았습니다. 추론하는 동안 최대 출력 길이를 입력 길이 + 300으로 늘 렸습니다. WSJ와 준 감독 설정 모두에 빔 크기 21과 α = 0.3을 사용했습니다. 표 4의 결과는 작업 별 튜닝이 없음에도 불구하고 우리 모델이 놀랍게도 잘 수행되어 Recurrent Neural Network Grammar [8]를 제외하고 이전에보고 된 모든 모델보다 더 나은 결과를 산출한다는 것을 보여줍니다. RNN 시퀀스-투-시퀀스 모델 [37]과 달리 Transformer는 40K 문장의 WSJ 학습 세트에서만 학습 할 때에도 BerkeleyParser [29]보다 성능이 뛰어납니다.

## 7 Conclusion
#### 이 작업에서 우리는 전적으로주의에 기반한 첫 번째 시퀀스 변환 모델 인 Transformer를 제시하여 인코더-디코더 아키텍처에서 가장 일반적으로 사용되는 반복 계층을 다중 방향 자기주의로 대체했습니다. 번역 작업의 경우 Transformer는 반복 또는 컨볼 루션 계층을 기반으로하는 아키텍처보다 훨씬 빠르게 훈련 될 수 있습니다. WMT 2014 영어-독일어 및 WMT 2014 영어-프랑스어 번역 작업에서 우리는 새로운 상태를 달성합니다. 이전 작업에서 우리의 최고의 모델은 이전에보고 된 모든 앙상블을 능가합니다. 우리는주의 기반 모델의 미래에 대해 기대하고 있으며이를 다른 작업에 적용 할 계획입니다. 트랜스포머를 텍스트 이외의 입력 및 출력 양식과 관련된 문제로 확장하고 이미지, 오디오 및 비디오와 같은 큰 입력 및 출력을 효율적으로 처리하기 위해 제한된 로컬주의 메커니즘을 조사 할 계획입니다. 세대를 덜 순차적으로 만드는 것은 우리의 또 다른 연구 목표입니다. 모델 학습 및 평가에 사용한 코드는 https://github.com/tensorflow/tensor2tensor에서 확인할 수 있습니다.

