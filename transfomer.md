# Transfomer

## Abstract

#### 지배적 인 시퀀스 변환 모델은 인코더와 디코더를 포함하는 복잡한 반복 또는 컨벌루션 신경망을 기반으로합니다. 최고 성능의 모델은 또한 어텐션 메커니즘을 통해 인코더와 디코더를 연결합니다. 우리는 어텐션 메커니즘만을 기반으로하는 새로운 단순 네트워크 아키텍처 인 Transformer를 제안합니다. 이는 전적으로 반복과 컨볼 루션을 제거합니다. 두 가지 기계 번역 작업에 대한 실험을 통해 이러한 모델은 품질이 우수하면서도 병렬화가 가능하고 학습 시간이 크게 단축되는 것으로 나타났습니다. 우리의 모델은 WMT 2014 영어-독일어 번역 작업에서 28.4 BLEU를 달성하여 앙상블을 포함한 기존 최고의 결과를 2BLEU 이상 향상 시켰습니다. WMT 2014 영어-프랑스어 번역 작업에서, 우리 모델은 8 개의 GPU에서 3.5 일 동안 훈련 한 후 새로운 단일 모델 최신 BLEU 점수 41.8을 설정합니다. 이는 최고의 훈련 비용의 일부입니다. 문학의 모델. Transformer가 크고 제한된 학습 데이터를 사용하여 영어 구성 구문 분석에 성공적으로 적용하여 다른 작업에 잘 일반화됨을 보여줍니다.


## 1 Introduction

#### 특히 반복 신경망, 장단기 기억 [13] 및 게이트 된 반복 신경망 [7]은 언어 모델링 및 기계 번역과 같은 시퀀스 모델링 및 변환 문제에서 최첨단 접근 방식으로 확고하게 확립되었습니다 [35, 2 , 5]. 그 이후로 반복되는 언어 모델과 인코더-디코더 아키텍처의 경계를 계속 확장하기위한 수많은 노력이 계속되었습니다 [38, 24, 15]. 반복 모델은 일반적으로 입력 및 출력 시퀀스의 기호 위치를 따라 계산을 인수 분해합니다. 계산 시간의 단계에 위치를 정렬하면 이전 숨겨진 상태 ht-1 및 위치 t에 대한 입력의 함수로 숨겨진 상태 시퀀스 ht를 생성합니다. 이러한 본질적으로 순차적 인 특성은 메모리 제약으로 인해 여러 예제에서 일괄 처리를 제한하기 때문에 긴 시퀀스 길이에서 중요 해지는 학습 예제 내에서 병렬화를 방지합니다. 최근 연구는 인수 분해 트릭 [21]과 조건부 계산 [32]을 통해 계산 효율성을 크게 향상 시켰으며 후자의 경우 모델 성능을 개선했습니다. 그러나 순차 계산의 근본적인 제약은 여전히 ​​남아 있습니다. 주의 메커니즘은 다양한 작업에서 강력한 시퀀스 모델링 및 변환 모델의 필수 부분이되어 입력 또는 출력 시퀀스의 거리에 관계없이 종속성을 모델링 할 수 있습니다 [2, 19]. 그러나 몇몇 경우를 제외한 모든 경우 [27], 이러한주의 메커니즘은 반복 네트워크와 함께 사용됩니다. 이 작업에서 우리는 반복을 피하고 대신에 전적으로주의 메커니즘에 의존하여 입력과 출력 사이의 전역 종속성을 그리는 모델 아키텍처 인 Transformer를 제안합니다. Transformer는 훨씬 더 많은 병렬화를 허용하며 8 개의 P100 GPU에서 12 시간 동안 교육을받은 후 번역 품질면에서 새로운 상태에 도달 할 수 있습니다.

## 2 Background

#### 순차 계산을 줄이는 목표는 확장 신경 GPU [16], ByteNet [18] 및 ConvS2S [9]의 기반을 형성합니다. 이들 모두는 컨볼 루션 신경망을 기본 빌딩 블록으로 사용하고 모든 입력 및 모든 입력에 대해 병렬로 숨겨진 표현을 계산합니다. 출력 위치. 이러한 모델에서 두 임의의 입력 또는 출력 위치에서 신호를 연결하는 데 필요한 작업 수는 위치 간 거리에서 ConvS2S의 경우 선형으로, ByteNet의 경우 대수적으로 증가합니다. 이것은 먼 위치 사이의 의존성을 배우는 것을 더 어렵게 만듭니다 [12]. Transformer에서는 평균주의 가중치 위치로 인해 유효 해상도가 감소하지만 섹션 3.2에 설명 된 것처럼 Multi-Head Attention에 대응하는 효과가 있지만 이는 일정한 수의 작업으로 감소됩니다. 내부주의라고도하는 자기주의는 시퀀스의 표현을 계산하기 위해 단일 시퀀스의 다른 위치와 관련된주의 메커니즘입니다. 자기주의는 독해, 추상적 인 요약, 텍스트 수반 및 학습 과제 독립적 문장 표현을 포함한 다양한 작업에서 성공적으로 사용되었습니다 [4, 27, 28, 22]. 엔드-투-엔드 메모리 네트워크는 순차 정렬 된 반복 대신 반복적 인주의 메커니즘을 기반으로하며 간단한 언어 질문 답변 및 언어 모델링 작업에서 잘 수행되는 것으로 나타났습니다 [34]. 그러나 우리가 아는 한, Transformer는 시퀀스 정렬 된 RNN 또는 컨볼 루션을 사용하지 않고 입력 및 출력의 표현을 계산하기 위해 전적으로 자기주의에 의존하는 최초의 변환 모델입니다. 다음 섹션에서는 Transformer에 대해 설명하고 자기주의를 유도하며 [17, 18] 및 [9]와 같은 모델에 비해 장점에 대해 논의합니다.

## 3 Model Architecture

#### 대부분의 경쟁적인 신경 시퀀스 변환 모델은 인코더-디코더 구조를 가지고 있습니다 [5, 2, 35]. 여기서 인코더는 기호 표현의 입력 시퀀스 (x1, ..., xn)를 연속 표현 시퀀스 z = (z1, ..., zn)에 매핑합니다. z가 주어지면 디코더는 한 번에 한 요소 씩 심볼의 출력 시퀀스 (y1, ..., ym)를 생성합니다. 각 단계에서 모델은 자동 회귀 (auto-regressive) [10]로 다음을 생성 할 때 이전에 생성 된 기호를 추가 입력으로 사용합니다. Transformer는 그림 1의 왼쪽 및 오른쪽 절반에 각각 표시된 인코더와 디코더 모두에 대해 누적 된 자기주의 및 포인트 방식의 완전 연결 계층을 사용하여이 전체 아키텍처를 따릅니다.

![image](https://user-images.githubusercontent.com/63130907/123031583-752ead00-d41f-11eb-93fc-684bfbe7d2e6.png)


## 3.1 Encoder and Decoder Stacks

#### 인코더 : 인코더는 N = 6 개의 동일한 레이어 스택으로 구성됩니다. 각 레이어에는 두 개의 하위 레이어가 있습니다. 첫 번째는 다중 헤드 자체주의 메커니즘이고 두 번째는 단순하고 위치별로 완전히 연결된 피드 포워드 네트워크입니다. 우리는 두 하위 계층 각각 주위에 잔류 연결 [11]을 사용하고 계층 정규화 [1]를 따릅니다. 즉, 각 하위 계층의 출력은 LayerNorm (x + Sublayer (x))이며, 여기서 Sublayer (x)는 하위 계층 자체에 의해 구현 된 함수입니다. 이러한 잔여 연결을 용이하게하기 위해 모델의 모든 하위 계층과 임베딩 계층은 차원 dmodel = 512의 출력을 생성합니다.

#### 디코더 : 디코더는 N = 6 개의 동일한 레이어 스택으로 구성됩니다. 각 인코더 레이어에있는 두 개의 하위 레이어 외에도 디코더는 인코더 스택의 출력에 대해 다중 헤드주의를 수행하는 세 번째 하위 레이어를 삽입합니다. 인코더와 마찬가지로 각 하위 계층 주변에 잔류 연결을 사용한 다음 계층 정규화를 사용합니다. 또한 디코더 스택의 자기주의 하위 계층을 수정하여 위치가 후속 위치에 집중하지 않도록합니다. 이 마스킹은 출력 임베딩이 한 위치만큼 오프셋된다는 사실과 결합되어 위치 i에 대한 예측이 i보다 작은 위치에서 알려진 출력에만 의존 할 수 있도록합니다.

## 3.2 Attention

#### 주의 함수는 쿼리와 키-값 쌍 집합을 출력에 매핑하는 것으로 설명 할 수 있습니다. 여기서 쿼리, 키, 값 및 출력은 모두 벡터입니다. 출력은 값의 가중 합계로 계산되며, 각 값에 할당 된 가중치는 해당 키와 쿼리의 호환성 함수에 의해 계산됩니다.

![image](https://user-images.githubusercontent.com/63130907/123031848-f71ed600-d41f-11eb-87b5-bc145a4fbd05.png)

## 3.2.1 Scaled Dot-Product Attention

#### 우리는 우리의 특별한 관심을 "Scaled Dot-Product Attention"이라고 부릅니다 (그림 2). 입력은 차원 dk의 쿼리 및 키와 차원 dv의 값으로 구성됩니다. 모든 키로 쿼리의 내적을 계산하고 각각을 √ dk로 나누고 소프트 맥스 함수를 적용하여 값에 대한 가중치를 얻습니다. 실제로 우리는 일련의 쿼리에 대한주의 함수를 동시에 계산하여 행렬 Q로 함께 압축합니다. 키와 값도 행렬 K 및 V로 함께 압축됩니다. 출력 행렬을 다음과 같이 계산합니다.

![image](https://user-images.githubusercontent.com/63130907/123031920-16b5fe80-d420-11eb-9c79-c6115ea4a30e.png)

#### 가장 일반적으로 사용되는 두 가지주의 함수는 덧셈주의 [2]와 내적 (곱셈)주의입니다. 내적주의는 √ 1 dk의 배율 인수를 제외하고는 우리 알고리즘과 동일합니다. 부가 적주의는 단일 히든 레이어가있는 피드 포워드 네트워크를 사용하여 호환성 함수를 계산합니다. 이 둘은 이론상의 복잡성이 비슷하지만 내적 관심은 고도로 최적화 된 행렬 곱셈 코드를 사용하여 구현할 수 있기 때문에 실제로 훨씬 빠르고 공간 효율적입니다. dk 값이 작은 경우 두 메커니즘이 비슷하게 수행되지만, dk 값이 더 큰 경우에는 가산 적주의가 내적주의를 능가합니다 [3]. 우리는 dk의 큰 값에 대해 내적의 크기가 커져서 소프트 맥스 함수를 극히 작은 기울기가있는 영역으로 밀어 넣는다 고 생각합니다 4. 이 효과에 대응하기 위해 내적을 √ 1 dk로 스케일합니다.

## 3.2.2 Multi-Head Attention

#### dmodel 차원 키, 값 및 쿼리를 사용하여 단일주의 기능을 수행하는 대신 학습 된 다른 선형 프로젝션을 사용하여 쿼리, 키 및 값을 각각 dk, dk 및 dv 차원으로 선형 프로젝션하는 것이 유리하다는 것을 알았습니다. 이러한 각 예상 버전의 쿼리, 키 및 값에서주의 기능을 병렬로 수행하여 dv 차원 출력 값을 산출합니다. 그림 2와 같이 연결되고 다시 투영되어 최종 값이 생성됩니다.

#### 다중 머리주의를 사용하면 모델이 서로 다른 위치에서 서로 다른 표현 부분 공간의 정보에 공동으로 참석할 수 있습니다. 하나의 어텐션 헤드로 평균화는 이것을 억제합니다.

![image](https://user-images.githubusercontent.com/63130907/123032106-60064e00-d420-11eb-8466-1112ec541427.png)

#### 투영이 매개 변수 행렬 인 경우 W Q i ∈ R dmodel × dk, W K i ∈ R dmodel × dk, WV i ∈ R dmodel × dv 및 WO ∈ R hdv × dmodel.
#### 이 작업에서 우리는 h = 8 개의 평행주의 레이어 또는 헤드를 사용합니다. 이들 각각에 대해 우리는 dk = dv = dmodel / h = 64를 사용합니다. 각 헤드의 감소 된 차원으로 인해 총 계산 비용은 완전한 차원의 단일 헤드주의 비용과 유사합니다.

## 3.2.3 Applications of Attention in our Model

#### Transformer는 세 가지 다른 방식으로 다중 머리주의를 사용합니다.
- "인코더-디코더주의"계층에서 쿼리는 이전 디코더 계층에서 제공되고 메모리 키 및 값은 인코더의 출력에서 제공됩니다. 이를 통해 디코더의 모든 위치가 입력 시퀀스의 모든 위치를 처리 할 수 있습니다. 이것은 [38, 2, 9]와 같은 sequence-to-sequence 모델에서 전형적인 인코더-디코더주의 메커니즘을 모방합니다.
- 인코더에는 자기주의 레이어가 포함되어 있습니다. 자기주의 계층에서 모든 키, 값 및 쿼리는 동일한 위치 (이 경우 인코더의 이전 계층 출력)에서 가져옵니다. 인코더의 각 위치는 인코더의 이전 계층에있는 모든 위치를 따를 수 있습니다.
- 유사하게, 디코더의 자기주의 계층은 디코더의 각 위치가 해당 위치를 포함하여 디코더의 모든 위치에 집중할 수 있도록합니다. 자동 회귀 속성을 유지하려면 디코더에서 왼쪽으로 정보 흐름을 방지해야합니다. 불법 연결에 해당하는 소프트 맥스의 입력에서 모든 값을 마스킹 (-∞로 설정)하여 스케일 된 내적주의 내부를 구현합니다. 그림 2 참조

## 3.3 Position-wise Feed-Forward Networks

#### 주의 하위 계층 외에도 인코더 및 디코더의 각 계층에는 완전히 연결된 피드 포워드 네트워크가 포함되어 있으며 각 위치에 개별적으로 동일하게 적용됩니다. 이것은 ReLU 활성화가 중간에있는 두 개의 선형 변환으로 구성됩니다. 

![image](https://user-images.githubusercontent.com/63130907/123033784-46b2d100-d423-11eb-85a5-4c3582c1d337.png)


#### 선형 변환은 다른 위치에서 동일하지만 레이어마다 다른 매개 변수를 사용합니다. 이것을 설명하는 또 다른 방법은 커널 크기가 1 인 두 컨볼 루션으로 입력 및 출력의 차원은 dmodel = 512이고 내부 계층의 차원은 df f = 2048입니다.

## 3.4 Embeddings and Softmax

#### 다른 시퀀스 변환 모델과 마찬가지로 학습 된 임베딩을 사용하여 입력 토큰과 출력 토큰을 차원 dmodel의 벡터로 변환합니다. 또한 일반적인 학습 선형 변환 및 softmax 함수를 사용하여 디코더 출력을 예측 된 다음 토큰 확률로 변환합니다. 우리 모델에서는 [30]과 유사하게 두 개의 임베딩 레이어와 pre-softmax 선형 변환 사이에 동일한 가중치 행렬을 공유합니다. 임베딩 레이어에서 가중치에 √ dmodel을 곱합니다.

## 3.5 Positional Encoding

#### 우리의 모델에는 반복과 컨볼 루션이 없기 때문에 모델이 시퀀스의 순서를 사용하려면 시퀀스에서 토큰의 상대적 또는 절대 위치에 대한 정보를 주입해야합니다. 이를 위해 인코더 및 디코더 스택의 하단에있는 입력 임베딩에 "위치 인코딩"을 추가합니다. 위치 인코딩은 임베딩과 동일한 차원 dmodel을 가지므로 둘을 합산 할 수 있습니다. 위치 인코딩의 많은 선택, 학습 및 고정이 있습니다 [9].

![image](https://user-images.githubusercontent.com/63130907/123034007-b1640c80-d423-11eb-83a0-d9018396da0a.png)

#### In this work, we use sine and cosine functions of different frequencies:

![image](https://user-images.githubusercontent.com/63130907/123034031-c2148280-d423-11eb-9835-412aab826e9c.png)

#### 여기서 pos는 위치이고 i는 차원입니다. 즉, 위치 인코딩의 각 차원은 정현파에 해당합니다. 파장은 2π에서 10000 · 2π로 기하학적 진행을 형성합니다. 고정 오프셋 k에 대해 P Epos + k는 P Epos의 선형 함수로 표현 될 수 있기 때문에 모델이 상대적 위치에 따라 쉽게 참석하는 방법을 배울 수 있다는 가설을 세웠 기 때문에이 함수를 선택했습니다. 또한 학습 된 위치 임베딩 [9]을 사용하여 실험 한 결과 두 버전이 거의 동일한 결과를 생성한다는 것을 발견했습니다 (표 3 행 (E) 참조). 모델이 훈련 중에 발생하는 길이보다 긴 시퀀스 길이로 외삽 할 수 있기 때문에 정현파 버전을 선택했습니다.

## 4 Why Self-Attention

#### 이 섹션에서는 자기주의 레이어의 다양한 측면을 심볼 표현의 하나의 가변 길이 시퀀스 (x1, ..., xn)를 동일한 길이의 다른 시퀀스 (z1, ..)에 매핑하는 데 일반적으로 사용되는 반복 및 컨볼 루션 레이어와 비교합니다. ., zn), xi, zi ∈ R d, 예를 들어 일반적인 시퀀스 변환 인코더 또는 디코더의 숨겨진 레이어. 자기주의를 사용하도록 동기를 부여하기 위해 세 가지 데시 데라 타를 고려합니다. 하나는 레이어 당 총 계산 복잡성입니다. 다른 하나는 필요한 최소 순차 작업 수로 측정 할 때 병렬화 할 수있는 계산량입니다. 세 번째는 네트워크에서 장거리 종속성 간의 경로 길이입니다. 장거리 의존성을 학습하는 것은 많은 시퀀스 변환 작업에서 핵심 과제입니다. 이러한 종속성을 학습하는 능력에 영향을 미치는 한 가지 핵심 요소는 순방향 및 역방향 신호가 네트워크에서 통과해야하는 경로의 길이입니다. 입력 및 출력 시퀀스의 위치 조합 사이의 이러한 경로가 짧을수록 장거리 종속성을 배우는 것이 더 쉽습니다 [12]. 따라서 다른 계층 유형으로 구성된 네트워크에서 두 입력 및 출력 위치 사이의 최대 경로 길이도 비교합니다. 표 1에서 알 수 있듯이 자기주의 계층은 모든 위치를 순차적으로 실행되는 일정한 수의 작업으로 연결하는 반면, 반복 계층은 O (n) 개의 연속 작업을 필요로합니다. 계산 복잡성 측면에서 자기주의 계층은 시퀀스 길이 n이 표현 차원 d보다 작을 때 반복 계층보다 빠릅니다. 이는 기계 번역의 최첨단 모델에서 사용되는 문장 표현에서 가장 자주 발생합니다. , 단어 조각 [38] 및 바이트 쌍 [31] 표현. 매우 긴 시퀀스를 포함하는 작업에 대한 계산 성능을 향상시키기 위해 각 출력 위치를 중심으로하는 입력 시퀀스에서 크기 r의 이웃 만 고려하는 것으로 자기주의를 제한 할 수 있습니다. 이것은 최대 경로 길이를 O (n / r)로 증가시킵니다. 향후 작업에서이 접근 방식을 추가로 조사 할 계획입니다. 커널 너비가 k <n 인 단일 컨벌루션 계층은 모든 쌍의 입력 및 출력 위치를 연결하지 않습니다. 이렇게하려면 연속 커널의 경우 O (n / k) 컨벌루션 레이어 스택이 필요하고 확장 된 컨볼 루션의 경우 O (logk (n)) [18], 두 위치 사이의 가장 긴 경로의 길이가 증가합니다. 네트워크에서. 컨볼 루션 레이어는 일반적으로 반복 레이어보다 k 배 더 비쌉니다. 그러나 분리 가능한 컨볼 루션 [6]은 복잡도를 O (k · n · d + n · d 2)로 상당히 감소시킵니다. 그러나 k = n을 사용하더라도 분리 가능한 컨볼 루션의 복잡성은 모델에서 취하는 접근 방식 인 자기주의 레이어와 포인트 단위 피드 포워드 레이어의 조합과 같습니다. 부수적 인 이점으로 자기주의는 더 해석 가능한 모델을 생성 할 수 있습니다. 우리는 모델의주의 분포를 검사하고 부록에서 예제를 제시하고 논의합니다. 개별주의 헤드는 서로 다른 작업을 수행하는 방법을 명확하게 배울뿐만 아니라 많은 문장의 구문 및 의미 구조와 관련된 행동을 보이는 것으로 보입니다.





## 

