# Transfomer

## Abstract

#### 지배적 인 시퀀스 변환 모델은 인코더와 디코더를 포함하는 복잡한 반복 또는 컨벌루션 신경망을 기반으로합니다. 최고 성능의 모델은 또한 어텐션 메커니즘을 통해 인코더와 디코더를 연결합니다. 우리는 어텐션 메커니즘만을 기반으로하는 새로운 단순 네트워크 아키텍처 인 Transformer를 제안합니다. 이는 전적으로 반복과 컨볼 루션을 제거합니다. 두 가지 기계 번역 작업에 대한 실험을 통해 이러한 모델은 품질이 우수하면서도 병렬화가 가능하고 학습 시간이 크게 단축되는 것으로 나타났습니다. 우리의 모델은 WMT 2014 영어-독일어 번역 작업에서 28.4 BLEU를 달성하여 앙상블을 포함한 기존 최고의 결과를 2BLEU 이상 향상 시켰습니다. WMT 2014 영어-프랑스어 번역 작업에서, 우리 모델은 8 개의 GPU에서 3.5 일 동안 훈련 한 후 새로운 단일 모델 최신 BLEU 점수 41.8을 설정합니다. 이는 최고의 훈련 비용의 일부입니다. 문학의 모델. Transformer가 크고 제한된 학습 데이터를 사용하여 영어 구성 구문 분석에 성공적으로 적용하여 다른 작업에 잘 일반화됨을 보여줍니다.


## 1 Introduction

#### 특히 반복 신경망, 장단기 기억 [13] 및 게이트 된 반복 신경망 [7]은 언어 모델링 및 기계 번역과 같은 시퀀스 모델링 및 변환 문제에서 최첨단 접근 방식으로 확고하게 확립되었습니다 [35, 2 , 5]. 그 이후로 반복되는 언어 모델과 인코더-디코더 아키텍처의 경계를 계속 확장하기위한 수많은 노력이 계속되었습니다 [38, 24, 15]. 반복 모델은 일반적으로 입력 및 출력 시퀀스의 기호 위치를 따라 계산을 인수 분해합니다. 계산 시간의 단계에 위치를 정렬하면 이전 숨겨진 상태 ht-1 및 위치 t에 대한 입력의 함수로 숨겨진 상태 시퀀스 ht를 생성합니다. 이러한 본질적으로 순차적 인 특성은 메모리 제약으로 인해 여러 예제에서 일괄 처리를 제한하기 때문에 긴 시퀀스 길이에서 중요 해지는 학습 예제 내에서 병렬화를 방지합니다. 최근 연구는 인수 분해 트릭 [21]과 조건부 계산 [32]을 통해 계산 효율성을 크게 향상 시켰으며 후자의 경우 모델 성능을 개선했습니다. 그러나 순차 계산의 근본적인 제약은 여전히 ​​남아 있습니다. 주의 메커니즘은 다양한 작업에서 강력한 시퀀스 모델링 및 변환 모델의 필수 부분이되어 입력 또는 출력 시퀀스의 거리에 관계없이 종속성을 모델링 할 수 있습니다 [2, 19]. 그러나 몇몇 경우를 제외한 모든 경우 [27], 이러한주의 메커니즘은 반복 네트워크와 함께 사용됩니다. 이 작업에서 우리는 반복을 피하고 대신에 전적으로주의 메커니즘에 의존하여 입력과 출력 사이의 전역 종속성을 그리는 모델 아키텍처 인 Transformer를 제안합니다. Transformer는 훨씬 더 많은 병렬화를 허용하며 8 개의 P100 GPU에서 12 시간 동안 교육을받은 후 번역 품질면에서 새로운 상태에 도달 할 수 있습니다.

## 2 Background

#### 순차 계산을 줄이는 목표는 확장 신경 GPU [16], ByteNet [18] 및 ConvS2S [9]의 기반을 형성합니다. 이들 모두는 컨볼 루션 신경망을 기본 빌딩 블록으로 사용하고 모든 입력 및 모든 입력에 대해 병렬로 숨겨진 표현을 계산합니다. 출력 위치. 이러한 모델에서 두 임의의 입력 또는 출력 위치에서 신호를 연결하는 데 필요한 작업 수는 위치 간 거리에서 ConvS2S의 경우 선형으로, ByteNet의 경우 대수적으로 증가합니다. 이것은 먼 위치 사이의 의존성을 배우는 것을 더 어렵게 만듭니다 [12]. Transformer에서는 평균주의 가중치 위치로 인해 유효 해상도가 감소하지만 섹션 3.2에 설명 된 것처럼 Multi-Head Attention에 대응하는 효과가 있지만 이는 일정한 수의 작업으로 감소됩니다. 내부주의라고도하는 자기주의는 시퀀스의 표현을 계산하기 위해 단일 시퀀스의 다른 위치와 관련된주의 메커니즘입니다. 자기주의는 독해, 추상적 인 요약, 텍스트 수반 및 학습 과제 독립적 문장 표현을 포함한 다양한 작업에서 성공적으로 사용되었습니다 [4, 27, 28, 22]. 엔드-투-엔드 메모리 네트워크는 순차 정렬 된 반복 대신 반복적 인주의 메커니즘을 기반으로하며 간단한 언어 질문 답변 및 언어 모델링 작업에서 잘 수행되는 것으로 나타났습니다 [34]. 그러나 우리가 아는 한, Transformer는 시퀀스 정렬 된 RNN 또는 컨볼 루션을 사용하지 않고 입력 및 출력의 표현을 계산하기 위해 전적으로 자기주의에 의존하는 최초의 변환 모델입니다. 다음 섹션에서는 Transformer에 대해 설명하고 자기주의를 유도하며 [17, 18] 및 [9]와 같은 모델에 비해 장점에 대해 논의합니다.

## 3 Model Architecture

#### 대부분의 경쟁적인 신경 시퀀스 변환 모델은 인코더-디코더 구조를 가지고 있습니다 [5, 2, 35]. 여기서 인코더는 기호 표현의 입력 시퀀스 (x1, ..., xn)를 연속 표현 시퀀스 z = (z1, ..., zn)에 매핑합니다. z가 주어지면 디코더는 한 번에 한 요소 씩 심볼의 출력 시퀀스 (y1, ..., ym)를 생성합니다. 각 단계에서 모델은 자동 회귀 (auto-regressive) [10]로 다음을 생성 할 때 이전에 생성 된 기호를 추가 입력으로 사용합니다. Transformer는 그림 1의 왼쪽 및 오른쪽 절반에 각각 표시된 인코더와 디코더 모두에 대해 누적 된 자기주의 및 포인트 방식의 완전 연결 계층을 사용하여이 전체 아키텍처를 따릅니다.

![image](https://user-images.githubusercontent.com/63130907/123031583-752ead00-d41f-11eb-93fc-684bfbe7d2e6.png)


## 
