# Transfomer

## Abstract

#### 지배적 인 시퀀스 변환 모델은 인코더와 디코더를 포함하는 복잡한 반복 또는 컨벌루션 신경망을 기반으로합니다. 최고 성능의 모델은 또한 어텐션 메커니즘을 통해 인코더와 디코더를 연결합니다. 우리는 어텐션 메커니즘만을 기반으로하는 새로운 단순 네트워크 아키텍처 인 Transformer를 제안합니다. 이는 전적으로 반복과 컨볼 루션을 제거합니다. 두 가지 기계 번역 작업에 대한 실험을 통해 이러한 모델은 품질이 우수하면서도 병렬화가 가능하고 학습 시간이 크게 단축되는 것으로 나타났습니다. 우리의 모델은 WMT 2014 영어-독일어 번역 작업에서 28.4 BLEU를 달성하여 앙상블을 포함한 기존 최고의 결과를 2BLEU 이상 향상 시켰습니다. WMT 2014 영어-프랑스어 번역 작업에서, 우리 모델은 8 개의 GPU에서 3.5 일 동안 훈련 한 후 새로운 단일 모델 최신 BLEU 점수 41.8을 설정합니다. 이는 최고의 훈련 비용의 일부입니다. 문학의 모델. Transformer가 크고 제한된 학습 데이터를 사용하여 영어 구성 구문 분석에 성공적으로 적용하여 다른 작업에 잘 일반화됨을 보여줍니다.


## 1 Introduction

#### 특히 반복 신경망, 장단기 기억 [13] 및 게이트 된 반복 신경망 [7]은 언어 모델링 및 기계 번역과 같은 시퀀스 모델링 및 변환 문제에서 최첨단 접근 방식으로 확고하게 확립되었습니다 [35, 2 , 5]. 그 이후로 반복되는 언어 모델과 인코더-디코더 아키텍처의 경계를 계속 확장하기위한 수많은 노력이 계속되었습니다 [38, 24, 15]. 반복 모델은 일반적으로 입력 및 출력 시퀀스의 기호 위치를 따라 계산을 인수 분해합니다. 계산 시간의 단계에 위치를 정렬하면 이전 숨겨진 상태 ht-1 및 위치 t에 대한 입력의 함수로 숨겨진 상태 시퀀스 ht를 생성합니다. 이러한 본질적으로 순차적 인 특성은 메모리 제약으로 인해 여러 예제에서 일괄 처리를 제한하기 때문에 긴 시퀀스 길이에서 중요 해지는 학습 예제 내에서 병렬화를 방지합니다. 최근 연구는 인수 분해 트릭 [21]과 조건부 계산 [32]을 통해 계산 효율성을 크게 향상 시켰으며 후자의 경우 모델 성능을 개선했습니다. 그러나 순차 계산의 근본적인 제약은 여전히 ​​남아 있습니다. 주의 메커니즘은 다양한 작업에서 강력한 시퀀스 모델링 및 변환 모델의 필수 부분이되어 입력 또는 출력 시퀀스의 거리에 관계없이 종속성을 모델링 할 수 있습니다 [2, 19]. 그러나 몇몇 경우를 제외한 모든 경우 [27], 이러한주의 메커니즘은 반복 네트워크와 함께 사용됩니다. 이 작업에서 우리는 반복을 피하고 대신에 전적으로주의 메커니즘에 의존하여 입력과 출력 사이의 전역 종속성을 그리는 모델 아키텍처 인 Transformer를 제안합니다. Transformer는 훨씬 더 많은 병렬화를 허용하며 8 개의 P100 GPU에서 12 시간 동안 교육을받은 후 번역 품질면에서 새로운 상태에 도달 할 수 있습니다.

## 2 Background

#### 순차 계산을 줄이는 목표는 확장 신경 GPU [16], ByteNet [18] 및 ConvS2S [9]의 기반을 형성합니다. 이들 모두는 컨볼 루션 신경망을 기본 빌딩 블록으로 사용하고 모든 입력 및 모든 입력에 대해 병렬로 숨겨진 표현을 계산합니다. 출력 위치. 이러한 모델에서 두 임의의 입력 또는 출력 위치에서 신호를 연결하는 데 필요한 작업 수는 위치 간 거리에서 ConvS2S의 경우 선형으로, ByteNet의 경우 대수적으로 증가합니다. 이것은 먼 위치 사이의 의존성을 배우는 것을 더 어렵게 만듭니다 [12]. Transformer에서는 평균주의 가중치 위치로 인해 유효 해상도가 감소하지만 섹션 3.2에 설명 된 것처럼 Multi-Head Attention에 대응하는 효과가 있지만 이는 일정한 수의 작업으로 감소됩니다. 내부주의라고도하는 자기주의는 시퀀스의 표현을 계산하기 위해 단일 시퀀스의 다른 위치와 관련된주의 메커니즘입니다. 자기주의는 독해, 추상적 인 요약, 텍스트 수반 및 학습 과제 독립적 문장 표현을 포함한 다양한 작업에서 성공적으로 사용되었습니다 [4, 27, 28, 22]. 엔드-투-엔드 메모리 네트워크는 순차 정렬 된 반복 대신 반복적 인주의 메커니즘을 기반으로하며 간단한 언어 질문 답변 및 언어 모델링 작업에서 잘 수행되는 것으로 나타났습니다 [34]. 그러나 우리가 아는 한, Transformer는 시퀀스 정렬 된 RNN 또는 컨볼 루션을 사용하지 않고 입력 및 출력의 표현을 계산하기 위해 전적으로 자기주의에 의존하는 최초의 변환 모델입니다. 다음 섹션에서는 Transformer에 대해 설명하고 자기주의를 유도하며 [17, 18] 및 [9]와 같은 모델에 비해 장점에 대해 논의합니다.

## 3 Model Architecture

#### 대부분의 경쟁적인 신경 시퀀스 변환 모델은 인코더-디코더 구조를 가지고 있습니다 [5, 2, 35]. 여기서 인코더는 기호 표현의 입력 시퀀스 (x1, ..., xn)를 연속 표현 시퀀스 z = (z1, ..., zn)에 매핑합니다. z가 주어지면 디코더는 한 번에 한 요소 씩 심볼의 출력 시퀀스 (y1, ..., ym)를 생성합니다. 각 단계에서 모델은 자동 회귀 (auto-regressive) [10]로 다음을 생성 할 때 이전에 생성 된 기호를 추가 입력으로 사용합니다. Transformer는 그림 1의 왼쪽 및 오른쪽 절반에 각각 표시된 인코더와 디코더 모두에 대해 누적 된 자기주의 및 포인트 방식의 완전 연결 계층을 사용하여이 전체 아키텍처를 따릅니다.

![image](https://user-images.githubusercontent.com/63130907/123031583-752ead00-d41f-11eb-93fc-684bfbe7d2e6.png)


## 3.1 Encoder and Decoder Stacks

#### 인코더 : 인코더는 N = 6 개의 동일한 레이어 스택으로 구성됩니다. 각 레이어에는 두 개의 하위 레이어가 있습니다. 첫 번째는 다중 헤드 자체주의 메커니즘이고 두 번째는 단순하고 위치별로 완전히 연결된 피드 포워드 네트워크입니다. 우리는 두 하위 계층 각각 주위에 잔류 연결 [11]을 사용하고 계층 정규화 [1]를 따릅니다. 즉, 각 하위 계층의 출력은 LayerNorm (x + Sublayer (x))이며, 여기서 Sublayer (x)는 하위 계층 자체에 의해 구현 된 함수입니다. 이러한 잔여 연결을 용이하게하기 위해 모델의 모든 하위 계층과 임베딩 계층은 차원 dmodel = 512의 출력을 생성합니다.

#### 디코더 : 디코더는 N = 6 개의 동일한 레이어 스택으로 구성됩니다. 각 인코더 레이어에있는 두 개의 하위 레이어 외에도 디코더는 인코더 스택의 출력에 대해 다중 헤드주의를 수행하는 세 번째 하위 레이어를 삽입합니다. 인코더와 마찬가지로 각 하위 계층 주변에 잔류 연결을 사용한 다음 계층 정규화를 사용합니다. 또한 디코더 스택의 자기주의 하위 계층을 수정하여 위치가 후속 위치에 집중하지 않도록합니다. 이 마스킹은 출력 임베딩이 한 위치만큼 오프셋된다는 사실과 결합되어 위치 i에 대한 예측이 i보다 작은 위치에서 알려진 출력에만 의존 할 수 있도록합니다.

## 3.2 Attention

#### 주의 함수는 쿼리와 키-값 쌍 집합을 출력에 매핑하는 것으로 설명 할 수 있습니다. 여기서 쿼리, 키, 값 및 출력은 모두 벡터입니다. 출력은 값의 가중 합계로 계산되며, 각 값에 할당 된 가중치는 해당 키와 쿼리의 호환성 함수에 의해 계산됩니다.

![image](https://user-images.githubusercontent.com/63130907/123031848-f71ed600-d41f-11eb-87b5-bc145a4fbd05.png)

## 3.2.1 Scaled Dot-Product Attention

#### 우리는 우리의 특별한 관심을 "Scaled Dot-Product Attention"이라고 부릅니다 (그림 2). 입력은 차원 dk의 쿼리 및 키와 차원 dv의 값으로 구성됩니다. 모든 키로 쿼리의 내적을 계산하고 각각을 √ dk로 나누고 소프트 맥스 함수를 적용하여 값에 대한 가중치를 얻습니다. 실제로 우리는 일련의 쿼리에 대한주의 함수를 동시에 계산하여 행렬 Q로 함께 압축합니다. 키와 값도 행렬 K 및 V로 함께 압축됩니다. 출력 행렬을 다음과 같이 계산합니다.

![image](https://user-images.githubusercontent.com/63130907/123031920-16b5fe80-d420-11eb-9c79-c6115ea4a30e.png)

#### 가장 일반적으로 사용되는 두 가지주의 함수는 덧셈주의 [2]와 내적 (곱셈)주의입니다. 내적주의는 √ 1 dk의 배율 인수를 제외하고는 우리 알고리즘과 동일합니다. 부가 적주의는 단일 히든 레이어가있는 피드 포워드 네트워크를 사용하여 호환성 함수를 계산합니다. 이 둘은 이론상의 복잡성이 비슷하지만 내적 관심은 고도로 최적화 된 행렬 곱셈 코드를 사용하여 구현할 수 있기 때문에 실제로 훨씬 빠르고 공간 효율적입니다. dk 값이 작은 경우 두 메커니즘이 비슷하게 수행되지만, dk 값이 더 큰 경우에는 가산 적주의가 내적주의를 능가합니다 [3]. 우리는 dk의 큰 값에 대해 내적의 크기가 커져서 소프트 맥스 함수를 극히 작은 기울기가있는 영역으로 밀어 넣는다 고 생각합니다 4. 이 효과에 대응하기 위해 내적을 √ 1 dk로 스케일합니다.

## 3.2.2 Multi-Head Attention

#### dmodel 차원 키, 값 및 쿼리를 사용하여 단일주의 기능을 수행하는 대신 학습 된 다른 선형 프로젝션을 사용하여 쿼리, 키 및 값을 각각 dk, dk 및 dv 차원으로 선형 프로젝션하는 것이 유리하다는 것을 알았습니다. 이러한 각 예상 버전의 쿼리, 키 및 값에서주의 기능을 병렬로 수행하여 dv 차원 출력 값을 산출합니다. 그림 2와 같이 연결되고 다시 투영되어 최종 값이 생성됩니다.

#### 다중 머리주의를 사용하면 모델이 서로 다른 위치에서 서로 다른 표현 부분 공간의 정보에 공동으로 참석할 수 있습니다. 하나의 어텐션 헤드로 평균화는 이것을 억제합니다.

![image](https://user-images.githubusercontent.com/63130907/123032106-60064e00-d420-11eb-8466-1112ec541427.png)

#### 투영이 매개 변수 행렬 인 경우 W Q i ∈ R dmodel × dk, W K i ∈ R dmodel × dk, WV i ∈ R dmodel × dv 및 WO ∈ R hdv × dmodel.
#### 이 작업에서 우리는 h = 8 개의 평행주의 레이어 또는 헤드를 사용합니다. 이들 각각에 대해 우리는 dk = dv = dmodel / h = 64를 사용합니다. 각 헤드의 감소 된 차원으로 인해 총 계산 비용은 완전한 차원의 단일 헤드주의 비용과 유사합니다.

## 3.2.3 Applications of Attention in our Model

#### Transformer는 세 가지 다른 방식으로 다중 머리주의를 사용합니다.
- "인코더-디코더주의"계층에서 쿼리는 이전 디코더 계층에서 제공되고 메모리 키 및 값은 인코더의 출력에서 제공됩니다. 이를 통해 디코더의 모든 위치가 입력 시퀀스의 모든 위치를 처리 할 수 있습니다. 이것은 [38, 2, 9]와 같은 sequence-to-sequence 모델에서 전형적인 인코더-디코더주의 메커니즘을 모방합니다.





## 

