# Transfomer

## 요약

#### 지배적 인 시퀀스 변환 모델은 인코더와 디코더를 포함하는 복잡한 반복 또는 컨벌루션 신경망을 기반으로합니다. 최고 성능의 모델은 또한 어텐션 메커니즘을 통해 인코더와 디코더를 연결합니다. 우리는 어텐션 메커니즘만을 기반으로하는 새로운 단순 네트워크 아키텍처 인 Transformer를 제안합니다. 이는 전적으로 반복과 컨볼 루션을 제거합니다. 두 가지 기계 번역 작업에 대한 실험을 통해 이러한 모델은 품질이 우수하면서도 병렬화가 가능하고 학습 시간이 크게 단축되는 것으로 나타났습니다. 우리의 모델은 WMT 2014 영어-독일어 번역 작업에서 28.4 BLEU를 달성하여 앙상블을 포함한 기존 최고의 결과를 2BLEU 이상 향상 시켰습니다. WMT 2014 영어-프랑스어 번역 작업에서, 우리 모델은 8 개의 GPU에서 3.5 일 동안 훈련 한 후 새로운 단일 모델 최신 BLEU 점수 41.8을 설정합니다. 이는 최고의 훈련 비용의 일부입니다. 문학의 모델. Transformer가 크고 제한된 학습 데이터를 사용하여 영어 구성 구문 분석에 성공적으로 적용하여 다른 작업에 잘 일반화됨을 보여줍니다.
